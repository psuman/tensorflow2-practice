{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.0.0-alpha0\n",
      "Eager execution is: True\n",
      "Keras version: 2.2.4-tf\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(\"TensorFlow version: {}\".format(tf.__version__))\n",
    "print(\"Eager execution is: {}\".format(tf.executing_eagerly()))\n",
    "print(\"Keras version: {}\".format(tf.keras.__version__))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running on GPU or CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on CPU\n"
     ]
    }
   ],
   "source": [
    "var = tf.Variable([3, 3])\n",
    "\n",
    "if tf.test.is_gpu_available(): \n",
    "    print('Running on GPU')\n",
    "    print('GPU #0?')\n",
    "    print(var.device.endswith('GPU:0'))\n",
    "else: \n",
    "    print('Running on CPU')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Declaring eager variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24,\n",
       " <tf.Variable 'Variable:0' shape=() dtype=int32, numpy=42>,\n",
       " <tf.Variable 'Variable:0' shape=(2, 2, 3) dtype=float32, numpy=\n",
       " array([[[ 0.,  1.,  2.],\n",
       "         [ 3.,  4.,  5.]],\n",
       " \n",
       "        [[ 6.,  7.,  8.],\n",
       "         [ 9., 10., 11.]]], dtype=float32)>)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t0 = 24 # python variable\n",
    "t1 = tf.Variable(42) # rank 0 tensor\n",
    "t2 = tf.Variable([ [ [0., 1., 2.], [3., 4., 5.] ], [ [6., 7., 8.], [9., 10., 11.] ] ]) #rank 3 tensor\n",
    "t0, t1, t2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tf.float64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f64 = tf.Variable(89, dtype = tf.float64)\n",
    "f64.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'Variable:0' shape=() dtype=float32, numpy=89.0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1 = tf.Variable(89.)\n",
    "f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=49, shape=(), dtype=int32, numpy=42>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m_o_l = tf.constant(42)\n",
    "m_o_l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shaping a tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 2, 3)\n"
     ]
    }
   ],
   "source": [
    "t2 = tf.Variable([ [ [0., 1., 2.], [3., 4., 5.] ], [ [6., 7., 8.], [9., 10., 11.] ] ]) # tensor variable\n",
    "print(t2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=60, shape=(2, 6), dtype=float32, numpy=\n",
       "array([[ 0.,  1.,  2.,  3.,  4.,  5.],\n",
       "       [ 6.,  7.,  8.,  9., 10., 11.]], dtype=float32)>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r1 = tf.reshape(t2,[2,6]) # 2 rows 6 cols\n",
    "r2 = tf.reshape(t2,[1,12]) # 1 rows 12 cols\n",
    "r1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ranking (dimensions) of a tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=66, shape=(), dtype=int32, numpy=3>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.rank(t2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specifying an element of a tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=72, shape=(), dtype=float32, numpy=8.0>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t3 = t2[1, 0, 2] # slice 1, row 0, column 2\n",
    "t3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Casting a tensor to a NumPy/Python variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[ 0.  1.  2.]\n",
      "  [ 3.  4.  5.]]\n",
      "\n",
      " [[ 6.  7.  8.]\n",
      "  [ 9. 10. 11.]]]\n"
     ]
    }
   ],
   "source": [
    "print(t2.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s =  tf.size(input=t2).numpy()\n",
    "s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specifying element-wise primitive tensor operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=81, shape=(2, 2, 3), dtype=float32, numpy=\n",
       "array([[[  0.,   1.,   4.],\n",
       "        [  9.,  16.,  25.]],\n",
       "\n",
       "       [[ 36.,  49.,  64.],\n",
       "        [ 81., 100., 121.]]], dtype=float32)>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t2*t2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Broadcasting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[[ 0.  4.  8.]\n",
      "  [12. 16. 20.]]\n",
      "\n",
      " [[24. 28. 32.]\n",
      "  [36. 40. 44.]]], shape=(2, 2, 3), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "t4 = t2*4\n",
    "print(t4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transposing TensorFlow and matrix multiplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=91, shape=(1, 1), dtype=int32, numpy=array([[14]], dtype=int32)>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "u = tf.constant([[3,4,3]]) \n",
    "v = tf.constant([[1,2,1]])\n",
    "tf.matmul(u, tf.transpose(a=v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=96, shape=(), dtype=int32, numpy=4>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "j = tf.cast(tf.constant(4.9), dtype=tf.int32) # 4j\n",
    "j"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Declaring ragged tensors\n",
    "\n",
    "A ragged tensor is a tensor with one or more ragged dimensions. Ragged dimensions are dimensions that have slices that may have different lengths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.RaggedTensor [[5, 2, 6, 1], [], [4, 10, 7], [8], [6, 7]]>\n",
      "tf.Tensor([5 2 6 1], shape=(4,), dtype=int32)\n",
      "tf.Tensor([], shape=(0,), dtype=int32)\n",
      "tf.Tensor([ 4 10  7], shape=(3,), dtype=int32)\n",
      "tf.Tensor([8], shape=(1,), dtype=int32)\n",
      "tf.Tensor([6 7], shape=(2,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "ragged =tf.ragged.constant([[5, 2, 6, 1], [], [4, 10, 7], [8], [6,7]])\n",
    "print(ragged)\n",
    "print(ragged[0,:])\n",
    "print(ragged[1,:])\n",
    "print(ragged[2,:])\n",
    "print(ragged[3,:])\n",
    "print(ragged[4,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Useful tensorflow operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=238, shape=(5,), dtype=int32, numpy=array([16,  4,  0,  4, 36], dtype=int32)>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = [1,3,5,7,11]\n",
    "y = 5\n",
    "s = tf.math.squared_difference(x,y)\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=242, shape=(), dtype=float32, numpy=4.75>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numbers = tf.constant([[4., 5.], [7., 3.]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=248, shape=(), dtype=float32, numpy=4.75>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.reduce_mean(input_tensor=numbers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=19, shape=(2,), dtype=float32, numpy=array([5.5, 4. ], dtype=float32)>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numbers = tf.constant([[4., 5.], [7., 3.]])\n",
    "tf.reduce_mean(input_tensor=numbers, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=23, shape=(1, 2), dtype=float32, numpy=array([[5.5, 4. ]], dtype=float32)>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numbers = tf.constant([[4., 5.], [7., 3.]])\n",
    "tf.reduce_mean(input_tensor=numbers, axis=0, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=35, shape=(2,), dtype=float32, numpy=array([4.5, 5. ], dtype=float32)>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numbers = tf.constant([[4., 5.], [7., 3.]])\n",
    "tf.reduce_mean(input_tensor=numbers, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=39, shape=(2, 1), dtype=float32, numpy=\n",
       "array([[4.5],\n",
       "       [5. ]], dtype=float32)>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numbers = tf.constant([[4., 5.], [7., 3.]])\n",
    "tf.reduce_mean(input_tensor=numbers, axis=1, keepdims=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating tensors filled with random values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[ 8.806503 10.29445 ]\n",
      " [ 8.745884 10.478781]\n",
      " [10.677351 11.253087]], shape=(3, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "ran = tf.random.normal(shape = (3,2), mean=10.0, stddev=2.0)\n",
    "print(ran)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=54, shape=(2, 4), dtype=float32, numpy=\n",
       "array([[0.98323774, 0.40994108, 0.1304717 , 0.7850783 ],\n",
       "       [0.32309508, 0.8220775 , 0.35015476, 0.31639576]], dtype=float32)>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.random.uniform(shape = (2,4),  minval=0, maxval=None, dtype=tf.float32, seed=None,  name=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## practical example of random values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[6 2 8]\n",
      " [3 6 9]\n",
      " [4 5 9]\n",
      " [2 3 5]\n",
      " [3 6 9]\n",
      " [3 3 6]\n",
      " [4 1 5]\n",
      " [1 1 2]\n",
      " [1 4 5]\n",
      " [1 1 2]], shape=(10, 3), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "dice1 = tf.Variable(tf.random.uniform([10, 1], minval=1, maxval=7, dtype=tf.int32))\n",
    "dice2 = tf.Variable(tf.random.uniform([10, 1], minval=1, maxval=7, dtype=tf.int32))\n",
    " \n",
    " # We may add dice1 and dice2 since they share the same shape and size.\n",
    "dice_sum = dice1 + dice2\n",
    " \n",
    " # We've got three separate 10x1 matrices. To produce a single\n",
    " # 10x3 matrix, we'll concatenate them along dimension 1.\n",
    "resulting_matrix = tf.concat(values=[dice1, dice2, dice_sum], axis=1)\n",
    " \n",
    "print(resulting_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding the indices of the largest and smallest element"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([  2  11   5  42   7  19  -6 -11  29], shape=(9,), dtype=int32)\n",
      "index of max;  tf.Tensor(3, shape=(), dtype=int64)\n",
      "Max element:  42\n",
      "index of min:  7\n",
      "Min element:  -11\n"
     ]
    }
   ],
   "source": [
    "t5 = tf.constant([2, 11, 5, 42, 7, 19, -6, -11, 29])\n",
    "print(t5)\n",
    "i = tf.argmax(input=t5)\n",
    "print('index of max; ', i)\n",
    "print('Max element: ', t5[i].numpy())\n",
    "i = tf.argmin(input=t5, axis=0).numpy()\n",
    "print('index of min: ', i)\n",
    "print('Min element: ',t5[i].numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[  2  11   5]\n",
      " [ 42   7  19]\n",
      " [ -6 -11  29]], shape=(3, 3), dtype=int32)\n",
      "indices of max down rows;  [1 0 2]\n",
      "indices of min down rows ;  [2 2 0]\n",
      "tf.Tensor(\n",
      "[[  2  11   5]\n",
      " [ 42   7  19]\n",
      " [ -6 -11  29]], shape=(3, 3), dtype=int32)\n",
      "indices of max across cols:  [1 0 2]\n",
      "indices of min across cols:  [0 1 1]\n"
     ]
    }
   ],
   "source": [
    "t6 = tf.reshape(t5, [3,3])\n",
    "print(t6)\n",
    "i = tf.argmax(input=t6,axis=0).numpy() # max arg down rows\n",
    "print('indices of max down rows; ', i)\n",
    "i = tf.argmin(input=t6,axis=0).numpy() # min arg down rows\n",
    "print('indices of min down rows ; ',i)\n",
    "print(t6)\n",
    "i = tf.argmax(input=t6,axis=1).numpy() # max arg across cols\n",
    "print('indices of max across cols: ',i)\n",
    "i = tf.argmin(input=t6,axis=1).numpy() # min arg across cols\n",
    "print('indices of min across cols: ',i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving and restoring tensor values using a checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'Variable:0' shape=(2, 4) dtype=int32, numpy=\n",
      "array([[ 1,  3,  5,  7],\n",
      "       [11, 13, 17, 19]], dtype=int32)>\n"
     ]
    }
   ],
   "source": [
    "variable = tf.Variable([[1,3,5,7],[11,13,17,19]])\n",
    "checkpoint= tf.train.Checkpoint(var=variable)\n",
    "save_path = checkpoint.save('./vars')\n",
    "variable.assign([[0,0,0,0],[0,0,0,0]])\n",
    "variable\n",
    "checkpoint.restore(save_path)\n",
    "print(variable)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using tf.function\n",
    "tf.function is a function that will take a Python function and return a TensorFlow graph. The advantage of this is that graphs can apply optimizations and exploit parallelism in the Python function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1(x, y):\n",
    "    return tf.reduce_mean(input_tensor=tf.multiply(x ** 2, 5) + y**2)\n",
    "\n",
    "f2 = tf.function(f1)\n",
    "\n",
    "x = tf.constant([4., -5.])\n",
    "y = tf.constant([2., 3.])\n",
    "\n",
    "# f1 and f2 return the same value, but f2 executes as a TensorFlow graph\n",
    "\n",
    "assert f1(x,y).numpy() == f2(x,y).numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keras, a High-Level API for TensorFlow 2\n",
    "\n",
    "Keras, which is a high-level API for TensorFlow 2.  Keras has become extremely popular for fast prototyping, for building and training deep learning models, and for research and production. Keras is a very rich API; it supports eager execution and data pipelines, and other features.\n",
    "\n",
    "Keras include built-in support for multi-GPU data parallelism, and also the fact that Keras models can be turned into TensorFlow Estimators and trained on clusters of GPUs on Google Cloud\n",
    "\n",
    "It's maintained independently of TensorFlow, although TensorFlow does have a full implementation of Keras in the tf.keras module. The implementation has TensorFlow-specific augmentations, including support for eager execution, by default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2.4-tf\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.keras.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=254, shape=(2, 2), dtype=float16, numpy=\n",
       "array([[42., 24.],\n",
       "       [11., 99.]], dtype=float16)>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras import backend as K\n",
    "const = K.constant([[42,24],[11,99]], dtype=tf.float16, shape=[2,2])\n",
    "const"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keras models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The first way to create a Sequential model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
      "11493376/11490434 [==============================] - 3s 0us/step\n",
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 5s 79us/sample - loss: 0.2210 - accuracy: 0.9351\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 4s 71us/sample - loss: 0.0970 - accuracy: 0.9703\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 5s 80us/sample - loss: 0.0682 - accuracy: 0.9786\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 5s 77us/sample - loss: 0.0530 - accuracy: 0.9828\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 8s 135us/sample - loss: 0.0440 - accuracy: 0.9854\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 4s 65us/sample - loss: 0.0381 - accuracy: 0.9873\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 4s 69us/sample - loss: 0.0318 - accuracy: 0.9894\n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - 9s 144us/sample - loss: 0.0259 - accuracy: 0.9918\n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - 6s 106us/sample - loss: 0.0245 - accuracy: 0.9917\n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - 6s 94us/sample - loss: 0.0222 - accuracy: 0.9926\n",
      "10000/10000 [==============================] - 0s 27us/sample - loss: 0.0692 - accuracy: 0.9822\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.0691536510806247, 0.9822]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist = tf.keras.datasets.mnist\n",
    "(train_x,train_y), (test_x, test_y) = mnist.load_data()\n",
    "\n",
    "epochs=10\n",
    "batch_size = 32 # 32 is default in fit method but specify anyway\n",
    "\n",
    "train_x, test_x = tf.cast(train_x/255.0, tf.float32), tf.cast(test_x/255.0, tf.float32)\n",
    "train_y, test_y = tf.cast(train_y,tf.int64),tf.cast(test_y,tf.int64) \n",
    "\n",
    "model1 = tf.keras.models.Sequential([\n",
    " tf.keras.layers.Flatten(),\n",
    " tf.keras.layers.Dense(512,activation=tf.nn.relu),\n",
    " tf.keras.layers.Dropout(0.2),\n",
    " tf.keras.layers.Dense(10,activation=tf.nn.softmax)\n",
    "])\n",
    "\n",
    "optimiser = tf.keras.optimizers.Adam()\n",
    "model1.compile (optimizer= optimiser, loss='sparse_categorical_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "model1.fit(train_x, train_y, batch_size=batch_size, epochs=epochs)\n",
    "\n",
    "model1.evaluate(test_x, test_y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The second way to create a Sequential model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 4s 64us/sample - loss: 0.2194 - accuracy: 0.9356\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 11s 185us/sample - loss: 0.0962 - accuracy: 0.9699\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 5s 90us/sample - loss: 0.0706 - accuracy: 0.9788\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 3s 56us/sample - loss: 0.0551 - accuracy: 0.9823\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 3s 56us/sample - loss: 0.0435 - accuracy: 0.9855\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 6s 101us/sample - loss: 0.0376 - accuracy: 0.9873\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 6s 108us/sample - loss: 0.0307 - accuracy: 0.9901\n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - 9s 146us/sample - loss: 0.0277 - accuracy: 0.9909\n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - 6s 98us/sample - loss: 0.0259 - accuracy: 0.9912\n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - 3s 57us/sample - loss: 0.0231 - accuracy: 0.9924\n",
      "10000/10000 [==============================] - 0s 21us/sample - loss: 0.0739 - accuracy: 0.9822\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.07390079730178914, 0.9822]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2 = tf.keras.models.Sequential();\n",
    "model2.add(tf.keras.layers.Flatten())\n",
    "model2.add(tf.keras.layers.Dense(512, activation='relu'))\n",
    "model2.add(tf.keras.layers.Dropout(0.2))\n",
    "model2.add(tf.keras.layers.Dense(10,activation=tf.nn.softmax))\n",
    "\n",
    "model2.compile (optimizer= tf.keras.optimizers.Adam(), loss='sparse_categorical_crossentropy', \n",
    " metrics = ['accuracy'])\n",
    "\n",
    "model2.fit(train_x, train_y, batch_size=batch_size, epochs=epochs)\n",
    "\n",
    "model2.evaluate(test_x, test_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Keras functional API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 4s 71us/sample - loss: 0.2228 - accuracy: 0.9333\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 4s 65us/sample - loss: 0.0965 - accuracy: 0.9699\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 4s 70us/sample - loss: 0.0689 - accuracy: 0.9788\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 4s 72us/sample - loss: 0.0543 - accuracy: 0.9826\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 4s 63us/sample - loss: 0.0452 - accuracy: 0.9856\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 4s 66us/sample - loss: 0.0362 - accuracy: 0.9879\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 6s 96us/sample - loss: 0.0312 - accuracy: 0.9896\n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - 7s 118us/sample - loss: 0.0283 - accuracy: 0.9903\n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - 4s 64us/sample - loss: 0.0253 - accuracy: 0.9917\n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - 4s 61us/sample - loss: 0.0221 - accuracy: 0.9926\n",
      "10000/10000 [==============================] - 0s 22us/sample - loss: 0.0705 - accuracy: 0.9823\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.07052516256404633, 0.9823]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "mnist = tf.keras.datasets.mnist\n",
    "(train_x,train_y), (test_x, test_y) = mnist.load_data()\n",
    "train_x, test_x = train_x/255.0, test_x/255.0\n",
    "epochs=10\n",
    "\n",
    "inputs = tf.keras.Input(shape=(28,28)) # Returns a 'placeholder' tensor\n",
    "\n",
    "x = tf.keras.layers.Flatten()(inputs)\n",
    "x = tf.keras.layers.Dense(512, activation='relu',name='d1')(x)\n",
    "x = tf.keras.layers.Dropout(0.2)(x)\n",
    "\n",
    "predictions = tf.keras.layers.Dense(10, activation=tf.nn.softmax, name='d2')(x)\n",
    "\n",
    "model3 = tf.keras.Model(inputs=inputs, outputs=predictions)\n",
    "\n",
    "optimiser = tf.keras.optimizers.Adam()\n",
    "model3.compile (optimizer= optimiser, loss='sparse_categorical_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "model3.fit(train_x, train_y, batch_size=32, epochs=epochs)\n",
    "\n",
    "model3.evaluate(test_x, test_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subclassing the Keras Model class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModel(tf.keras.Model):\n",
    " def __init__(self, num_classes=10):\n",
    "   super(MyModel, self).__init__()\n",
    "   self.inputs = tf.keras.Input(shape=(28,28)) # Returns a placeholder tensor\n",
    "   self.x0 = tf.keras.layers.Flatten()\n",
    "   self.x1 = tf.keras.layers.Dense(512, activation='relu',name='d1')\n",
    "   self.x2 = tf.keras.layers.Dropout(0.2)\n",
    "   self.predictions = tf.keras.layers.Dense(10,activation=tf.nn.softmax, name='d2')\n",
    "\n",
    " def call(self, inputs):\n",
    " # This is where to define your forward pass\n",
    " # using the layers previously defined in `__init__`\n",
    "   x = self.x0(self.inputs)\n",
    "   x = self.x1(x)\n",
    "   x = self.x2(x) \n",
    "   return self.predictions(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Using a `tf.Tensor` as a Python `bool` is not allowed. Use `if t is not None:` instead of `if t:` to test if a tensor is defined, and use TensorFlow ops such as tf.cond to execute subgraphs conditioned on the value of a tensor.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-46-f907892190d9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m model4.compile (optimizer= tf.keras.optimizers.Adam(), loss='sparse_categorical_crossentropy', \n\u001b[1;32m      3\u001b[0m  metrics = ['accuracy'])\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mmodel4\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mmodel4\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/workspaces/tensorflow2-practice/.venv/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    804\u001b[0m         \u001b[0msteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    805\u001b[0m         \u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_split\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 806\u001b[0;31m         shuffle=shuffle)\n\u001b[0m\u001b[1;32m    807\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    808\u001b[0m     \u001b[0;31m# Prepare validation data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/workspaces/tensorflow2-practice/.venv/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, batch_size, check_steps, steps_name, steps, validation_split, shuffle, extract_tensors_from_dataset)\u001b[0m\n\u001b[1;32m   2461\u001b[0m     \u001b[0;31m# rather than list inputs (e.g. FeatureColumn-based models).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2462\u001b[0m     \u001b[0mdict_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2463\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2464\u001b[0m       \u001b[0;31m# We need to use `x_input` to set the model inputs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2465\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/workspaces/tensorflow2-practice/.venv/lib/python3.7/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m__bool__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    658\u001b[0m       \u001b[0;31m`\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    659\u001b[0m     \"\"\"\n\u001b[0;32m--> 660\u001b[0;31m     raise TypeError(\"Using a `tf.Tensor` as a Python `bool` is not allowed. \"\n\u001b[0m\u001b[1;32m    661\u001b[0m                     \u001b[0;34m\"Use `if t is not None:` instead of `if t:` to test if a \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    662\u001b[0m                     \u001b[0;34m\"tensor is defined, and use TensorFlow ops such as \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Using a `tf.Tensor` as a Python `bool` is not allowed. Use `if t is not None:` instead of `if t:` to test if a tensor is defined, and use TensorFlow ops such as tf.cond to execute subgraphs conditioned on the value of a tensor."
     ]
    }
   ],
   "source": [
    "model4 = MyModel()\n",
    "model4.compile (optimizer= tf.keras.optimizers.Adam(), loss='sparse_categorical_crossentropy', metrics = ['accuracy'])\n",
    "model4.fit(train_x, train_y, batch_size=32, epochs=10)\n",
    "model4.evaluate(test_x, test_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using data pipelines\n",
    "\n",
    "Data may also be passed into the fit method as a tf.data.Dataset() iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "10/10 [==============================] - 1s 53ms/step - loss: 5.0150 - accuracy: 0.5813\n",
      "Epoch 2/10\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 2.3393 - accuracy: 0.7500\n",
      "Epoch 3/10\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 2.9338 - accuracy: 0.6062\n",
      "Epoch 4/10\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 1.6812 - accuracy: 0.7437\n",
      "Epoch 5/10\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 1.0459 - accuracy: 0.8000\n",
      "Epoch 6/10\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 1.1154 - accuracy: 0.7750\n",
      "Epoch 7/10\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 1.0910 - accuracy: 0.8031\n",
      "Epoch 8/10\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.8977 - accuracy: 0.8031\n",
      "Epoch 9/10\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.8256 - accuracy: 0.8250\n",
      "Epoch 10/10\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.5918 - accuracy: 0.8500\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f25fc366f98>"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 32\n",
    "buffer_size = 10000\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((train_x, train_y)).batch(32).shuffle(10000)\n",
    "\n",
    "train_dataset = train_dataset.map(lambda x, y: (tf.image.random_flip_left_right(x), y))\n",
    "train_dataset = train_dataset.repeat()\n",
    "\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((test_x, test_y)).batch(batch_size).shuffle(10000)\n",
    "test_dataset = train_dataset.repeat()\n",
    "\n",
    "steps_per_epoch = len(train_x)//batch_size # required because of the repeat on the dataset\n",
    "optimiser = tf.keras.optimizers.Adam()\n",
    "model3.compile (optimizer= optimiser, loss='sparse_categorical_crossentropy', metrics = ['accuracy'])\n",
    "model3.fit(train_dataset, epochs=10, steps_per_epoch=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving and loading Keras models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "model3.save('./model_name.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "new_model = load_model('./model_name.h5')\n",
    "\n",
    "model3.save_weights('./model_weights.h5')\n",
    "model3.load_weights('./model_weights.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keras datasets\n",
    "\n",
    "The following datasets are available from within Keras: boston_housing, cifar10, cifar100, fashion_mnist, imdb, mnist, and reuters.\n",
    "\n",
    "They are all accessed with the load_data() function. For example, to load the fashion_mnist dataset, use the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
      "32768/29515 [=================================] - 0s 1us/step\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
      "26427392/26421880 [==============================] - 4s 0us/step\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
      "8192/5148 [===============================================] - 0s 0us/step\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
      "4423680/4422102 [==============================] - 1s 0us/step\n"
     ]
    }
   ],
   "source": [
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.fashion_mnist.load_data()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ANN Technologies Using TensorFlow 2\n",
    "\n",
    "Using NumPy arrays with datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One shot Iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np \n",
    "\n",
    "num_items = 11\n",
    "num_list1 = np.arange(num_items)\n",
    "num_list2 = np.arange(num_items,num_items*2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "num_list1_dataset = tf.data.Dataset.from_tensor_slices(num_list1)\n",
    "iterator = tf.compat.v1.data.make_one_shot_iterator(num_list1_dataset)\n",
    "for item in num_list1_dataset:\n",
    "    num = iterator.get_next().numpy()\n",
    "    print(num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One shot Iterator with batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2]\n",
      "[3 4 5]\n",
      "[6 7 8]\n",
      "[ 9 10]\n"
     ]
    }
   ],
   "source": [
    "num_list1_dataset = tf.data.Dataset.from_tensor_slices(num_list1).batch(3, drop_remainder = False)\n",
    "iterator = tf.compat.v1.data.make_one_shot_iterator(num_list1_dataset)\n",
    "for item in num_list1_dataset:\n",
    "    num = iterator.get_next().numpy()\n",
    "    print(num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One shot Iterator with Zipped dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<tf.Tensor: id=96, shape=(), dtype=int32, numpy=1>, <tf.Tensor: id=97, shape=(), dtype=string, numpy=b'a'>)\n",
      "(<tf.Tensor: id=102, shape=(), dtype=int32, numpy=2>, <tf.Tensor: id=103, shape=(), dtype=string, numpy=b'e'>)\n",
      "(<tf.Tensor: id=108, shape=(), dtype=int32, numpy=3>, <tf.Tensor: id=109, shape=(), dtype=string, numpy=b'i'>)\n",
      "(<tf.Tensor: id=114, shape=(), dtype=int32, numpy=4>, <tf.Tensor: id=115, shape=(), dtype=string, numpy=b'o'>)\n",
      "(<tf.Tensor: id=120, shape=(), dtype=int32, numpy=5>, <tf.Tensor: id=121, shape=(), dtype=string, numpy=b'u'>)\n"
     ]
    }
   ],
   "source": [
    "dataset1 = [1,2,3,4,5]\n",
    "dataset2 = ['a','e','i','o','u']\n",
    "dataset1 = tf.data.Dataset.from_tensor_slices(dataset1)\n",
    "dataset2 = tf.data.Dataset.from_tensor_slices(dataset2)\n",
    "zipped_datasets = tf.data.Dataset.zip((dataset1, dataset2))\n",
    "iterator = tf.compat.v1.data.make_one_shot_iterator(zipped_datasets)\n",
    "for item in zipped_datasets:\n",
    "    num = iterator.get_next()\n",
    "    print(num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One shot Iterator with Concatenate datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<ConcatenateDataset shapes: (), types: tf.int32>\n",
      "tf.Tensor(1, shape=(), dtype=int32)\n",
      "tf.Tensor(2, shape=(), dtype=int32)\n",
      "tf.Tensor(3, shape=(), dtype=int32)\n",
      "tf.Tensor(5, shape=(), dtype=int32)\n",
      "tf.Tensor(7, shape=(), dtype=int32)\n",
      "tf.Tensor(11, shape=(), dtype=int32)\n",
      "tf.Tensor(13, shape=(), dtype=int32)\n",
      "tf.Tensor(17, shape=(), dtype=int32)\n",
      "tf.Tensor(19, shape=(), dtype=int32)\n",
      "tf.Tensor(23, shape=(), dtype=int32)\n",
      "tf.Tensor(29, shape=(), dtype=int32)\n",
      "tf.Tensor(31, shape=(), dtype=int32)\n",
      "tf.Tensor(37, shape=(), dtype=int32)\n",
      "tf.Tensor(41, shape=(), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "ds1 = tf.data.Dataset.from_tensor_slices([1,2,3,5,7,11,13,17])\n",
    "ds2 = tf.data.Dataset.from_tensor_slices([19,23,29,31,37,41])\n",
    "ds3 = ds1.concatenate(ds2)\n",
    "print(ds3)\n",
    "iterator = tf.compat.v1.data.make_one_shot_iterator(ds3)\n",
    "for i in range(14):\n",
    "  num = iterator.get_next()\n",
    "  print(num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CSV example 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NotFoundError",
     "evalue": "./size_1000.csv; No such file or directory [Op:IteratorGetNextSync]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotFoundError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-f44f6ec9bd33>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mrecord_defaults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;31m# two required float columns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCsvDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecord_defaults\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mselect_cols\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/workspaces/tensorflow2-practice/.venv/lib/python3.7/site-packages/tensorflow/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    554\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    555\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# For Python 3 compatibility\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 556\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    557\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    558\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_next_internal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/workspaces/tensorflow2-practice/.venv/lib/python3.7/site-packages/tensorflow/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36mnext\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    583\u001b[0m     \"\"\"\n\u001b[1;32m    584\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 585\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_internal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    586\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    587\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/workspaces/tensorflow2-practice/.venv/lib/python3.7/site-packages/tensorflow/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36m_next_internal\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    575\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_resource\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    576\u001b[0m             \u001b[0moutput_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flat_output_types\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 577\u001b[0;31m             output_shapes=self._flat_output_shapes)\n\u001b[0m\u001b[1;32m    578\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    579\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_structure\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_from_compatible_tensor_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/workspaces/tensorflow2-practice/.venv/lib/python3.7/site-packages/tensorflow/python/ops/gen_dataset_ops.py\u001b[0m in \u001b[0;36miterator_get_next_sync\u001b[0;34m(iterator, output_types, output_shapes, name)\u001b[0m\n\u001b[1;32m   1952\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1953\u001b[0m         \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1954\u001b[0;31m       \u001b[0m_six\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1955\u001b[0m   \u001b[0;31m# Add nodes to the TensorFlow graph.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1956\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_types\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/workspaces/tensorflow2-practice/.venv/lib/python3.7/site-packages/six.py\u001b[0m in \u001b[0;36mraise_from\u001b[0;34m(value, from_value)\u001b[0m\n",
      "\u001b[0;31mNotFoundError\u001b[0m: ./size_1000.csv; No such file or directory [Op:IteratorGetNextSync]"
     ]
    }
   ],
   "source": [
    "filename = [\"./size_1000.csv\"]\n",
    "record_defaults = [tf.float32] * 2 # two required float columns\n",
    "dataset = tf.data.experimental.CsvDataset(filename, record_defaults, header=True, select_cols=[1,2])\n",
    "for item in dataset:\n",
    "  print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CSV example 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NotFoundError",
     "evalue": "mycsvfile.txt; No such file or directory [Op:IteratorGetNextSync]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotFoundError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-b549333dd30d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mrecord_defaults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0.0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint32\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCsvDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecord_defaults\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mselect_cols\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/workspaces/tensorflow2-practice/.venv/lib/python3.7/site-packages/tensorflow/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    554\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    555\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# For Python 3 compatibility\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 556\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    557\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    558\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_next_internal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/workspaces/tensorflow2-practice/.venv/lib/python3.7/site-packages/tensorflow/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36mnext\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    583\u001b[0m     \"\"\"\n\u001b[1;32m    584\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 585\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_internal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    586\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    587\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/workspaces/tensorflow2-practice/.venv/lib/python3.7/site-packages/tensorflow/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36m_next_internal\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    575\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_resource\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    576\u001b[0m             \u001b[0moutput_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flat_output_types\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 577\u001b[0;31m             output_shapes=self._flat_output_shapes)\n\u001b[0m\u001b[1;32m    578\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    579\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_structure\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_from_compatible_tensor_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/workspaces/tensorflow2-practice/.venv/lib/python3.7/site-packages/tensorflow/python/ops/gen_dataset_ops.py\u001b[0m in \u001b[0;36miterator_get_next_sync\u001b[0;34m(iterator, output_types, output_shapes, name)\u001b[0m\n\u001b[1;32m   1952\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1953\u001b[0m         \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1954\u001b[0;31m       \u001b[0m_six\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1955\u001b[0m   \u001b[0;31m# Add nodes to the TensorFlow graph.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1956\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_types\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/workspaces/tensorflow2-practice/.venv/lib/python3.7/site-packages/six.py\u001b[0m in \u001b[0;36mraise_from\u001b[0;34m(value, from_value)\u001b[0m\n",
      "\u001b[0;31mNotFoundError\u001b[0m: mycsvfile.txt; No such file or directory [Op:IteratorGetNextSync]"
     ]
    }
   ],
   "source": [
    "#file Chapter_2.ipynb\n",
    "filename = \"mycsvfile.txt\"\n",
    "record_defaults = [tf.float32, tf.constant([0.0], dtype=tf.float32), tf.int32,]\n",
    "dataset = tf.data.experimental.CsvDataset(filename, record_defaults, header=False, select_cols=[1,2,3])\n",
    "for item in dataset:\n",
    "  print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CSV example 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NotFoundError",
     "evalue": "file1.txt; No such file or directory [Op:IteratorGetNextSync]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotFoundError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-2ba2ae86409d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mrecord_defaults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstring\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCsvDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecord_defaults\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# decode as string is in binary format.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/workspaces/tensorflow2-practice/.venv/lib/python3.7/site-packages/tensorflow/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    554\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    555\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# For Python 3 compatibility\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 556\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    557\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    558\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_next_internal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/workspaces/tensorflow2-practice/.venv/lib/python3.7/site-packages/tensorflow/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36mnext\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    583\u001b[0m     \"\"\"\n\u001b[1;32m    584\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 585\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_internal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    586\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    587\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/workspaces/tensorflow2-practice/.venv/lib/python3.7/site-packages/tensorflow/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36m_next_internal\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    575\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_resource\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    576\u001b[0m             \u001b[0moutput_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flat_output_types\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 577\u001b[0;31m             output_shapes=self._flat_output_shapes)\n\u001b[0m\u001b[1;32m    578\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    579\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_structure\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_from_compatible_tensor_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/workspaces/tensorflow2-practice/.venv/lib/python3.7/site-packages/tensorflow/python/ops/gen_dataset_ops.py\u001b[0m in \u001b[0;36miterator_get_next_sync\u001b[0;34m(iterator, output_types, output_shapes, name)\u001b[0m\n\u001b[1;32m   1952\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1953\u001b[0m         \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1954\u001b[0;31m       \u001b[0m_six\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1955\u001b[0m   \u001b[0;31m# Add nodes to the TensorFlow graph.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1956\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_types\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/workspaces/tensorflow2-practice/.venv/lib/python3.7/site-packages/six.py\u001b[0m in \u001b[0;36mraise_from\u001b[0;34m(value, from_value)\u001b[0m\n",
      "\u001b[0;31mNotFoundError\u001b[0m: file1.txt; No such file or directory [Op:IteratorGetNextSync]"
     ]
    }
   ],
   "source": [
    "filename = \"file1.txt\"\n",
    "record_defaults = [tf.float32, tf.float32, tf.string ,]\n",
    "dataset = tf.data.experimental.CsvDataset(filename, record_defaults, header=False)\n",
    "for item in dataset:\n",
    "    print(item[0].numpy(), item[1].numpy(),item[2].numpy().decode() ) \n",
    "# decode as string is in binary format."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TFRecords\n",
    "\n",
    "Another popular choice for storing data is the TFRecord format. This is a binary file format. For large files, it is a good choice because binary files take up less disc space, take less time to copy, and can be read very efficiently from the disc.\n",
    "\n",
    "Because a TFRecord file is a sequence of binary strings, its structure must be specified prior to saving so that it can be properly written and subsequently read back. \n",
    "\n",
    "TensorFlow has two structures for this, tf.train.Example and tf.train.SequenceExample. \n",
    "\n",
    "What you have to do is store each sample of your data in one of these structures, then serialize it, and use tf.python_io.TFRecordWriter to save it to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "data=np.array([10.,11.,12.,13.,14.,15.])\n",
    "\n",
    "def npy_to_tfrecords(fname,data):\n",
    "    writer = tf.io.TFRecordWriter(fname)\n",
    "    feature={}\n",
    "    feature['data'] = tf.train.Feature(float_list=tf.train.FloatList(value=data))\n",
    "    example = tf.train.Example(features=tf.train.Features(feature=feature))\n",
    "    serialized = example.SerializeToString()\n",
    "    writer.write(serialized)\n",
    "    writer.close()\n",
    "\n",
    "npy_to_tfrecords(\"./myfile.tfrecords\",data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([10. 11. 12. 13. 14. 15.], shape=(6,), dtype=float32)\n",
      "[10. 11. 12. 13. 14. 15.]\n",
      "12.0\n"
     ]
    }
   ],
   "source": [
    "dataset = tf.data.TFRecordDataset(\"./myfile.tfrecords\")\n",
    "\n",
    "def parse_function(example_proto):\n",
    "    keys_to_features = {'data':tf.io.FixedLenSequenceFeature([], dtype = tf.float32, allow_missing = True) }\n",
    "    parsed_features = tf.io.parse_single_example(serialized=example_proto, features=keys_to_features)\n",
    "    return parsed_features['data']\n",
    "\n",
    "dataset = dataset.map(parse_function)\n",
    "iterator = tf.compat.v1.data.make_one_shot_iterator(dataset)\n",
    "# array is retrieved as one item\n",
    "item = iterator.get_next()\n",
    "print(item)\n",
    "print(item.numpy())\n",
    "print(item[2].numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TFRecord example 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = './students.tfrecords'\n",
    "data = {\n",
    "            'ID': 61553,\n",
    "            'Name': ['Jones', 'Felicity'],\n",
    "            'Scores': [45.6, 97.2] \n",
    "        }\n",
    "\n",
    "ID = tf.train.Feature(int64_list=tf.train.Int64List(value=[data['ID']]))\n",
    "\n",
    "Name = tf.train.Feature(bytes_list=tf.train.BytesList(value=[n.encode('utf-8') for n in data['Name']]))\n",
    "\n",
    "Scores = tf.train.Feature(float_list=tf.train.FloatList(value=data['Scores']))\n",
    "\n",
    "example = tf.train.Example(features=tf.train.Features(feature={'ID': ID, 'Name': Name, 'Scores': Scores }))\n",
    "\n",
    "writer = tf.io.TFRecordWriter(filename)\n",
    "writer.write(example.SerializeToString())\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tf.data.TFRecordDataset(\"./students.tfrecords\")\n",
    "\n",
    "def parse_function(example_proto):\n",
    "    keys_to_features = {'ID':tf.io.FixedLenFeature([], dtype = tf.int64),\n",
    "                       'Name':tf.io.VarLenFeature(dtype = tf.string),\n",
    "                        'Scores':tf.io.VarLenFeature(dtype = tf.float32)\n",
    "                       }\n",
    "    parsed_features = tf.io.parse_single_example(serialized=example_proto, features=keys_to_features)\n",
    "    return parsed_features[\"ID\"], parsed_features[\"Name\"],parsed_features[\"Scores\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<tf.Tensor: id=309, shape=(), dtype=int64, numpy=61553>, <tensorflow.python.framework.sparse_tensor.SparseTensor object at 0x7fcb500486d8>, <tensorflow.python.framework.sparse_tensor.SparseTensor object at 0x7fcb247f5630>)\n"
     ]
    }
   ],
   "source": [
    "dataset = dataset.map(parse_function)\n",
    "\n",
    "iterator = tf.compat.v1.data.make_one_shot_iterator(dataset)\n",
    "item = iterator.get_next()\n",
    "# record is retrieved as one item\n",
    "print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID:  61553\n",
      "Name: Jones , Felicity\n",
      "Scores:  [45.6 97.2]\n"
     ]
    }
   ],
   "source": [
    "print(\"ID: \",item[0].numpy())\n",
    "name = item[1].values.numpy()\n",
    "name1= name[0].decode()\n",
    "name2 = name[1].decode('utf8')\n",
    "print(\"Name:\",name1,\",\",name2)\n",
    "print(\"Scores: \",item[2].values.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One-hot encoding\n",
    "\n",
    "One-hot encoding (OHE) is where a tensor is constructed from the data labels with a 1 in each of the elements corresponding to a label's value, and 0 everywhere else; that is, one of the bits in the tensor is hot (1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 is  [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.] when one-hot encoded with a depth of 10\n"
     ]
    }
   ],
   "source": [
    "y = 5\n",
    "y_train_ohe = tf.one_hot(y, depth=10).numpy() \n",
    "print(y, \"is \",y_train_ohe,\"when one-hot encoded with a depth of 10\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "2\n",
      "[0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.python.keras.datasets import fashion_mnist\n",
    "tf.executing_eagerly()\n",
    "width, height, = 28,28\n",
    "n_classes = 10\n",
    "\n",
    "# load the dataset\n",
    "(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()\n",
    "split = 50000\n",
    "#split feature training set into training and validation sets\n",
    "(y_train, y_valid) = y_train[:split], y_train[split:]\n",
    "\n",
    "# one-hot encode the labels using TensorFlow. \n",
    "# then convert back to numpy for display \n",
    "\n",
    "y_train_ohe = tf.one_hot(y_train, depth=n_classes).numpy() \n",
    "y_valid_ohe = tf.one_hot(y_valid, depth=n_classes).numpy()\n",
    "y_test_ohe = tf.one_hot(y_test, depth=n_classes).numpy()\n",
    "\n",
    "# show difference between the original label and a one-hot-encoded label\n",
    "\n",
    "i=5\n",
    "print(y_train[i]) # 'ordinary' number value of label at index i=5 is 2\n",
    "# 2\n",
    "# note the difference between the index of 5 and the label at that index which is 2\n",
    "print(y_train_ohe[i]) # \n",
    "# 0. 0. 1. 0. 0.0 .0 .0. 0. 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layers\n",
    "\n",
    "The fundamental data structure used by ANNs is the layer, and many interconnected layers make up a complete ANN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dense (fully connected) layer\n",
    "A dense layer is a fully connected layer. This means that all neurons in the previous layer are connected to all neurons in the next layer. In a dense network, all layers are dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer = tf.keras.layers.Dense(n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional layer\n",
    "\n",
    "A convolutional layer is a layer where the neurons in a layer are grouped into small patches by the use of a filter, which is usually square, and created by sliding the filter over the layer. Each patch is convolved, that is, multiplied and summed by the filter. Convolutional nets or ConvNets for short, have proved themselves to be very good at image recognition and manipulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "seqtial_Net = tf.keras.Sequential([tf.keras.layers.Conv2D(1, (1, 1), strides = 1, padding='valid')])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Max pooling layer\n",
    "\n",
    "A max pooling layer takes the maximum value within its window as the window slides over the layer, in much the same way as a convolution takes place.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.layers.MaxPooling2D(pool_size=(2, 2), strides=None, padding='valid', data_format=None)\n",
    "layer = tf.keras.layers.MaxPooling2D()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch normalization layer and dropout layer\n",
    "\n",
    "Batch normalization is a layer that takes its inputs and outputs the same number of outputs with activations that have zero mean and unit variance, as this has been found to be beneficial to learning. Batch normalization regulates the activations so that they neither become vanishingly small nor explosively big, both of which situations prevent the network from learning.\n",
    "\n",
    "Dropout layer is a layer where a certain percentage of the neurons are randomly turned off during training (not during inference). This forces the network to become better at generalizing since individual neurons are discouraged from becoming specialized with respect to their inputs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.layers.BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001, center=True, scale=True, beta_initializer='zeros', gamma_initializer='ones', moving_mean_initializer='zeros', moving_variance_initializer='ones', beta_regularizer=None, gamma_regularizer=None, beta_constraint=None, gamma_constraint=None)\n",
    "layer = tf.keras.layers.BatchNormalization()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "rate=0.5\n",
    "tf.keras.layers.Dropout(rate, noise_shape=None, seed=None)\n",
    "layer = tf.keras.layers.Dropout(rate = 0.5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Softmax layer\n",
    "\n",
    "A softmax layer is a layer where the activation of each output unit corresponds to the probability that the output unit matches a given label. The output neuron with the highest activation value is, therefore, the prediction of the net. It is used when the classes being learned are mutually exclusive, so that the probabilities output by the softmax layer total 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.layers.core.Dense at 0x7fcb247f6240>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.keras.layers.Dense(10,activation=tf.nn.softmax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activation functions\n",
    "\n",
    "It is important to note that neural nets have non-linear activation functions, that is, the functions that are applied to the sum of the weighted inputs to a neuron. Linear activation units are not able to map input layers to output layers in all but trivial neural net models.\n",
    "\n",
    "There are a number of activation functions in common use, including sigmoid, tanh, ReLU, and leaky ReLU. A good summary, together with diagrams of these functions, may be found here: https://towardsdatascience.com/activation-functions-neural-networks-1cbd9f8d91d6."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the model\n",
    "\n",
    "There are four methods for creating our ANN model using Keras:\n",
    "\n",
    "Method 1: Arguments passed to tf.keras.Sequential\n",
    "Method 2: Use of the .add method of tf.keras.Sequential\n",
    "Method 3: Use of the Keras functional API\n",
    "Method 4: By subclassing the tf.keras.Model object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient calculations for gradient descent algorithms\n",
    "\n",
    "One of TenorFlow's great strengths is its ability to automatically compute gradients for use in gradient descent algorithms, which, of course, are a vital part of most machine learning models. TensorFlow offers a number of methods for gradient calculations.\n",
    "\n",
    "tf.GradientTape: Context records computations so that you can call tf.gradient() to get the gradients of any tensor computed while recording with respect to any trainable variable\n",
    "\n",
    "tfe.gradients_function(): Takes a function (say f()) and returns a gradient function (say fg()) that can compute the gradients of the outputs of f() with respect to the parameters of f() or a subset of them\n",
    "\n",
    "tfe.implicit_gradients(): This is very similar, but fg() computes the gradients of the outputs of f() with regard to all trainable variables that these outputs depend on\n",
    "\n",
    "tfe.implicit_value_and_gradients(): This is almost identical, but fg() also returns the output of the function, f()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.0\n"
     ]
    }
   ],
   "source": [
    "# by default, you can only call tape.gradient once in a GradientTape context\n",
    "weight1 = tf.Variable(2.0)\n",
    "\n",
    "def weighted_sum(x1):\n",
    "   return weight1 * x1\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "   sum = weighted_sum(7.)\n",
    "   [weight1_grad] = tape.gradient(sum, [weight1])\n",
    "    \n",
    "print(weight1_grad.numpy()) # 7 , weight1*x diff w.r.t. weight1 is x, 7.0, also see below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.0\n",
      "5.0\n",
      "6.0\n"
     ]
    }
   ],
   "source": [
    "# if you need to call tape.gradient() more than once\n",
    "# use GradientTape(persistent=True) \n",
    "weight1 = tf.Variable(2.0)\n",
    "weight2 = tf.Variable(3.0)\n",
    "weight3 = tf.Variable(5.0)\n",
    "\n",
    "def weighted_sum(x1, x2, x3):\n",
    "    return weight1*x1 + weight2*x2 + weight3*x3\n",
    "\n",
    "with tf.GradientTape(persistent=True) as tape:\n",
    "   sum = weighted_sum(7.,5.,6.)\n",
    "\n",
    "[weight1_grad] = tape.gradient(sum, [weight1])\n",
    "[weight2_grad] = tape.gradient(sum, [weight2])\n",
    "[weight3_grad] = tape.gradient(sum, [weight3])\n",
    "\n",
    "print(weight1_grad.numpy()) #7.0\n",
    "print(weight2_grad.numpy()) #5.0\n",
    "print(weight3_grad.numpy()) #6.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss functions\n",
    "\n",
    "A loss function (that is, an error measurement) is a necessary part of the training of an ANN. It is a measure of the extent to which the calculated output of a network during training differs from its required output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
